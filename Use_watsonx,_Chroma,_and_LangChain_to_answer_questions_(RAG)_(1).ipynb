{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "c_uuzpsGKvgS"
      },
      "source": [
        "### Install and import the dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "mls_NvcGKvgU",
        "outputId": "72b8cbda-c359-4f74-a834-da5a28bc9a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed wget-3.2\n",
            "Successfully installed langchain-0.3.13 langchain-core-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "plotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ibm-cos-sdk-2.13.6 ibm-cos-sdk-core-2.13.6 ibm-cos-sdk-s3transfer-2.13.6 ibm_watsonx_ai-1.1.26 jmespath-1.0.1 lomond-0.3.3 pandas-2.1.4 python-dateutil-2.9.0.post0 requests-2.32.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "d7c141170e194ca0a06c97241fdcd06d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed langchain_ibm-0.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "transformers 4.47.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 langchain_chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.2 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 tokenizers-0.20.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install wget | tail -n 1\n",
        "!pip install -U \"langchain>=0.3,<0.4\" | tail -n 1\n",
        "!pip install -U \"ibm_watsonx_ai>=1.1.22\" | tail -n 1\n",
        "!pip install -U \"langchain_ibm>=0.3,<0.4\" | tail -n 1\n",
        "!pip install -U \"langchain_chroma>=0.1,<0.2\" | tail -n 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Z0h0sbmMKvgZ"
      },
      "outputs": [],
      "source": [
        "import os, getpass"
      ]
    },
    {
      "source": [
        "import os\n",
        "import getpass # Correctly import the getpass module\n",
        "\n",
        "from ibm_watsonx_ai import Credentials\n",
        "\n",
        "credentials = Credentials(\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    api_key=getpass.getpass(\"Please enter your WML api key (hit enter): \"),\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNJZl8BoLm2W",
        "outputId": "43aaf60c-9969-41a6-c0b5-3a79f92d91ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your WML api key (hit enter): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L0HwcwOKvgd",
        "outputId": "801c84b7-d281-4b64-df95-66dd79c2ec3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your project_id (hit enter): fffad246-8e82-4926-bc22-b17b6d943097\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    project_id = os.environ[\"PROJECT_ID\"]\n",
        "except KeyError:\n",
        "    project_id = input(\"Please enter your project_id (hit enter): \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbeJbe4VKvge"
      },
      "source": [
        "Create an instance of APIClient with authentication details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zp3owOZkKvgf"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai import APIClient\n",
        "\n",
        "api_client = APIClient(credentials=credentials, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hP4JVUOxKvgg"
      },
      "source": [
        "<a id=\"data\"></a>\n",
        "## Document data loading\n",
        "\n",
        "Download the file with State of the Union."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tvquJOF1Kvgh"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "\n",
        "filename = 'RAG_QA.pdf'\n",
        "# url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "    wget.download(url, out=filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEl7sKcKKvgi",
        "outputId": "4f8618cf-9e7c-42d2-e14e-b8ef85ccd682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.9)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma<0.2,>=0.1) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install -U \"langchain_chroma>=0.1,<0.2\" | tail -n 1 # Install the correct package 'langchain_chroma' instead of 'langchain-community'.\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "loader = TextLoader(filename)\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LKfaIPb1Kvgj"
      },
      "source": [
        "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCRpRtYdKvgk",
        "outputId": "fd90351e-41f3-4474-c9b1-812086b7531e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'SLATE_125M_ENGLISH_RTRVR': 'ibm/slate-125m-english-rtrvr', 'SLATE_125M_ENGLISH_RTRVR_V2': 'ibm/slate-125m-english-rtrvr-v2', 'SLATE_30M_ENGLISH_RTRVR': 'ibm/slate-30m-english-rtrvr', 'SLATE_30M_ENGLISH_RTRVR_V2': 'ibm/slate-30m-english-rtrvr-v2', 'MULTILINGUAL_E5_LARGE': 'intfloat/multilingual-e5-large', 'ALL_MINILM_L12_V2': 'sentence-transformers/all-minilm-l12-v2', 'ALL_MINILM_L6_V2': 'sentence-transformers/all-minilm-l6-v2'}\n"
          ]
        }
      ],
      "source": [
        "api_client.foundation_models.EmbeddingModels.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob50HP_pKvgn"
      },
      "outputs": [],
      "source": [
        "from langchain_ibm import WatsonxEmbeddings\n",
        "\n",
        "embeddings = WatsonxEmbeddings(\n",
        "    model_id=\"ibm/slate-30m-english-rtrvr\",\n",
        "    url=credentials[\"url\"],\n",
        "    apikey=credentials[\"apikey\"],\n",
        "    project_id=project_id\n",
        "    )\n",
        "docsearch = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuD1Cxc7Kvgs",
        "outputId": "e67deca7-c471-4d67-bb59-87a9cfd798ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class WatsonxEmbeddings in module langchain_ibm.embeddings:\n",
            "\n",
            "class WatsonxEmbeddings(pydantic.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
            " |  WatsonxEmbeddings(*, model_id: str, project_id: Optional[str] = None, space_id: Optional[str] = None, url: pydantic.types.SecretStr = None, apikey: Optional[pydantic.types.SecretStr] = None, token: Optional[pydantic.types.SecretStr] = None, password: Optional[pydantic.types.SecretStr] = None, username: Optional[pydantic.types.SecretStr] = None, instance_id: Optional[pydantic.types.SecretStr] = None, version: Optional[pydantic.types.SecretStr] = None, params: Optional[Dict] = None, verify: Union[str, bool, NoneType] = None, watsonx_embed: ibm_watsonx_ai.foundation_models.embeddings.embeddings.Embeddings = None, watsonx_client: Optional[ibm_watsonx_ai.client.APIClient] = None) -> None\n",
            " |  \n",
            " |  IBM watsonx.ai embedding models.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      WatsonxEmbeddings\n",
            " |      pydantic.main.BaseModel\n",
            " |      langchain_core.embeddings.embeddings.Embeddings\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  embed_documents(self, texts: List[str], **kwargs: Any) -> List[List[float]]\n",
            " |      Embed search docs.\n",
            " |  \n",
            " |  embed_query(self, text: str, **kwargs: Any) -> List[float]\n",
            " |      Embed query text.\n",
            " |  \n",
            " |  validate_environment(self) -> typing_extensions.Self\n",
            " |      Validate that credentials and python package exists in environment.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'apikey': typing.Optional[pydantic.types.SecretStr]...\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __pydantic_complete__ = True\n",
            " |  \n",
            " |  __pydantic_core_schema__ = {'function': {'function': <function Watsonx...\n",
            " |  \n",
            " |  __pydantic_custom_init__ = False\n",
            " |  \n",
            " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
            " |  \n",
            " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
            " |  \n",
            " |  __pydantic_parent_namespace__ = None\n",
            " |  \n",
            " |  __pydantic_post_init__ = None\n",
            " |  \n",
            " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
            " |      Model...\n",
            " |  \n",
            " |  __pydantic_validator__ = SchemaValidator(title=\"WatsonxEmbeddings\", va...\n",
            " |  \n",
            " |  __signature__ = <Signature (*, model_id: str, project_id: Option...ibm...\n",
            " |  \n",
            " |  model_computed_fields = {}\n",
            " |  \n",
            " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'forbid', 'p...\n",
            " |  \n",
            " |  model_fields = {'apikey': FieldInfo(annotation=Union[SecretStr, NoneTy...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __copy__(self) -> 'Self'\n",
            " |      Returns a shallow copy of the model.\n",
            " |  \n",
            " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
            " |      Returns a deep copy of the model.\n",
            " |  \n",
            " |  __delattr__(self, item: 'str') -> 'Any'\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __eq__(self, other: 'Any') -> 'bool'\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __getattr__(self, item: 'str') -> 'Any'\n",
            " |  \n",
            " |  __getstate__(self) -> 'dict[Any, Any]'\n",
            " |  \n",
            " |  __init__(self, /, **data: 'Any') -> 'None'\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
            " |      validated to form a valid model.\n",
            " |      \n",
            " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      So `dict(model)` works.\n",
            " |  \n",
            " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
            " |  \n",
            " |  __repr__(self) -> 'str'\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
            " |  \n",
            " |  __repr_name__(self) -> 'str'\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
            " |  \n",
            " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
            " |  \n",
            " |  __str__(self) -> 'str'\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Returns a copy of the model.\n",
            " |      \n",
            " |      !!! warning \"Deprecated\"\n",
            " |          This method is now deprecated; use `model_copy` instead.\n",
            " |      \n",
            " |      If you need `include` or `exclude`, use:\n",
            " |      \n",
            " |      ```py\n",
            " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
            " |      data = {**data, **(update or {})}\n",
            " |      copied = self.model_validate(data)\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
            " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
            " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
            " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A copy of the model with included, excluded and updated fields as specified.\n",
            " |  \n",
            " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
            " |  \n",
            " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
            " |  \n",
            " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#model_copy\n",
            " |      \n",
            " |      Returns a copy of the model.\n",
            " |      \n",
            " |      Args:\n",
            " |          update: Values to change/add in the new model. Note: the data is not validated\n",
            " |              before creating the new model. You should trust this data.\n",
            " |          deep: Set to `True` to make a deep copy of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          New model instance.\n",
            " |  \n",
            " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump\n",
            " |      \n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode: The mode in which `to_python` should run.\n",
            " |              If mode is 'json', the output will only contain JSON serializable types.\n",
            " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
            " |          include: A set of fields to include in the output.\n",
            " |          exclude: A set of fields to exclude from the output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dictionary representation of the model.\n",
            " |  \n",
            " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump_json\n",
            " |      \n",
            " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
            " |      \n",
            " |      Args:\n",
            " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
            " |          include: Field(s) to include in the JSON output.\n",
            " |          exclude: Field(s) to exclude from the JSON output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to serialize using field aliases.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string representation of the model.\n",
            " |  \n",
            " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
            " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
            " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Hook into generating the model's CoreSchema.\n",
            " |      \n",
            " |      Args:\n",
            " |          source: The class we are generating a schema for.\n",
            " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
            " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `pydantic-core` `CoreSchema`.\n",
            " |  \n",
            " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Hook into generating the model's JSON schema.\n",
            " |      \n",
            " |      Args:\n",
            " |          core_schema: A `pydantic-core` CoreSchema.\n",
            " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
            " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
            " |              or just call the handler with the original schema.\n",
            " |          handler: Call into Pydantic's internal JSON schema generation.\n",
            " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
            " |              generation fails.\n",
            " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
            " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
            " |              for a type.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON schema, as a Python object.\n",
            " |  \n",
            " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
            " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
            " |      be present when this is called.\n",
            " |      \n",
            " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
            " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
            " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
            " |      \n",
            " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
            " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
            " |              by pydantic.\n",
            " |  \n",
            " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Creates a new instance of the `Model` class with validated data.\n",
            " |      \n",
            " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      \n",
            " |      !!! note\n",
            " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
            " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
            " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
            " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
            " |          an error if extra values are passed, but they will be ignored.\n",
            " |      \n",
            " |      Args:\n",
            " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
            " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
            " |              Otherwise, the field names from the `values` argument will be used.\n",
            " |          values: Trusted or pre-validated data dictionary.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new instance of the `Model` class with validated data.\n",
            " |  \n",
            " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Generates a JSON schema for a model class.\n",
            " |      \n",
            " |      Args:\n",
            " |          by_alias: Whether to use attribute aliases or not.\n",
            " |          ref_template: The reference template.\n",
            " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
            " |              `GenerateJsonSchema` with your desired modifications\n",
            " |          mode: The mode in which to generate the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The JSON schema for the given model class.\n",
            " |  \n",
            " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Compute the class name for parametrizations of generic classes.\n",
            " |      \n",
            " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
            " |      \n",
            " |      Args:\n",
            " |          params: Tuple of types of the class. Given a generic class\n",
            " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
            " |              the value `(str, int)` would be passed to `params`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
            " |      \n",
            " |      Raises:\n",
            " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
            " |  \n",
            " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Try to rebuild the pydantic-core schema for the model.\n",
            " |      \n",
            " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
            " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
            " |      \n",
            " |      Args:\n",
            " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
            " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
            " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
            " |          _types_namespace: The types namespace, defaults to `None`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
            " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
            " |  \n",
            " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Validate a pydantic model instance.\n",
            " |      \n",
            " |      Args:\n",
            " |          obj: The object to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          from_attributes: Whether to extract data from object attributes.\n",
            " |          context: Additional context to pass to the validator.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValidationError: If the object could not be validated.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated model instance.\n",
            " |  \n",
            " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/json/#json-parsing\n",
            " |      \n",
            " |      Validate the given JSON data against the Pydantic model.\n",
            " |      \n",
            " |      Args:\n",
            " |          json_data: The JSON data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
            " |  \n",
            " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Validate the given object with string data against the Pydantic model.\n",
            " |      \n",
            " |      Args:\n",
            " |          obj: The object containing string data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |  \n",
            " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  model_extra\n",
            " |      Get extra fields set during validation.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
            " |  \n",
            " |  model_fields_set\n",
            " |      Returns the set of fields that have been explicitly set on this model instance.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A set of strings representing the fields that have been set,\n",
            " |              i.e. that were not filled from defaults.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __pydantic_extra__\n",
            " |  \n",
            " |  __pydantic_fields_set__\n",
            " |  \n",
            " |  __pydantic_private__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __pydantic_root_model__ = False\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.embeddings.embeddings.Embeddings:\n",
            " |  \n",
            " |  async aembed_documents(self, texts: list[str]) -> list[list[float]]\n",
            " |      Asynchronous Embed search docs.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: List of text to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of embeddings.\n",
            " |  \n",
            " |  async aembed_query(self, text: str) -> list[float]\n",
            " |      Asynchronous Embed query text.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: Text to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Embedding.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(WatsonxEmbeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nC1JtFRsKvgs"
      },
      "source": [
        "<a id=\"models\"></a>\n",
        "## Foundation Models on `watsonx.ai`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YsFr7Yu9Kvgt"
      },
      "source": [
        "### Defining model\n",
        "You need to specify `model_id` that will be used for inferencing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RxIfyym5Kvgu"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
        "\n",
        "model_id = ModelTypes.GRANITE_13B_CHAT_V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "mHdnSTJHKvgv"
      },
      "source": [
        "### Defining the model parameters\n",
        "We need to provide a set of model parameters that will influence the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FovmkynPKvgv"
      },
      "outputs": [],
      "source": [
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
        "\n",
        "parameters = {\n",
        "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
        "    GenParams.MIN_NEW_TOKENS: 1,\n",
        "    GenParams.MAX_NEW_TOKENS: 100,\n",
        "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCSLxJclKvgv"
      },
      "source": [
        "### LangChain CustomLLM wrapper for watsonx model\n",
        "Initialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55UucYBqKvgw",
        "outputId": "09a9bba7-4c71-4584-e2ae-b6e84d94c561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'ibm/granite-13b-chat-v2' is in deprecated state from 2024-11-04 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_ibm import WatsonxLLM\n",
        "\n",
        "watsonx_granite = WatsonxLLM(\n",
        "    model_id=model_id.value,\n",
        "    url=credentials.get(\"url\"),\n",
        "    apikey=credentials.get(\"apikey\"),\n",
        "    project_id=project_id,\n",
        "    params=parameters\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "seGhpFQMKvgw"
      },
      "source": [
        "<a id=\"predict\"></a>\n",
        "## Generate a retrieval-augmented response to a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z1-3yS2Kvgx"
      },
      "source": [
        "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJOTpNyZKvgy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G9bhh5ZpKvgy"
      },
      "source": [
        "### Select questions\n",
        "\n",
        "Get questions from the previously loaded test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXL6r7M_Kvgz",
        "outputId": "92b06dbb-afd2-4633-c105-65cc74807b4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
              " 'result': ' The president said, \"One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\" This statement was made in reference to Ketanji Brown Jackson, who was nominated by the president to serve on the United States Supreme Court.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RkAka6WEKvg1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "217Cs1DgKvg2"
      },
      "source": [
        "<a id=\"summary\"></a>\n",
        "## Summary and next steps\n",
        "\n",
        " You successfully completed this notebook!.\n",
        "\n",
        " You learned how to answer question using RAG using watsonx and LangChain.\n",
        "\n",
        "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "aCjxOHiVKvg3"
      },
      "source": [
        "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "notebook-samples",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}